---
title: "Beyond Flat Walks: Compositional Abstraction for Autoregressive Molecular Generation"
authors:
  - name: Kaiwen Bian
  - name: Andrew H. Yang
  - name: Ali Parviz
  - name: Gal Mishne
  - name: Yusu Wang
links:
  - name: Paper
    url: "#"
    icon: ri:file-pdf-2-line
  - name: Code
    url: https://github.com/KevinBian107/MOSAIC
    icon: ri:github-line
theme: light
favicon: favicon.svg
description: "MOSAIC: Compositional Abstraction for Autoregressive Molecular Generation"
---

import HighlightedSection from "./components/HighlightedSection.astro";
import Figure from "./components/Figure.astro";
import Picture from "./components/Picture.astro";
import Wide from "./components/Wide.astro";
import pipelineOverview from "./assets/pipeline_overview.png";
import coarsening from "./assets/coarsening.png";
import tokenization from "./assets/tokenization.png";
import constraint from "./assets/constraint.png";
import galleryMoses from "./assets/generation_gallery_moses.png";

<HighlightedSection>

## Abstract

Autoregressive models for molecular graph generation typically operate on flattened sequences of atoms and bonds, discarding the rich multi-scale structure inherent to molecules. We introduce **MOSAIC** (**M**ulti-scale **O**rganization via **S**tructural **A**bstraction **I**n **C**omposition), a framework that lifts autoregressive generation from flat token walks to compositional, hierarchy-aware sequences. MOSAIC provides a unified three-stage pipeline: (1) **hierarchical coarsening** that recursively groups atoms into motif-like clusters using graph-theoretic methods (spectral clustering, hierarchical agglomerative clustering, and motif-aware variants), (2) **structured tokenization** that serializes the resulting multi-level hierarchy into sequences that explicitly encode parent-child relationships, partition boundaries, and edge connectivity at every level, and (3) **autoregressive generation** with a standard Transformer decoder that learns to produce these structured sequences. We evaluate MOSAIC on the MOSES and COCONUT molecular benchmarks, comparing four tokenization schemes of increasing hierarchical expressiveness. Our experiments show that hierarchy-aware tokenizations improve chemical validity and structural diversity over flat baselines while enabling control over generated substructures. MOSAIC provides a principled, modular foundation for structure-aware molecular generation.

</HighlightedSection>

## Introduction

Discovering new molecules — for drugs, materials, or chemical tools — is a slow and expensive process. Recent advances in generative AI offer a way to accelerate this: train a model on known molecules and let it propose entirely new ones. But molecules aren't just sequences of characters. They have rich internal structure: rings, branches, and recurring building blocks that determine their chemical properties.

Most existing approaches flatten a molecular graph into a linear sequence of atoms and bonds, then generate one token at a time — much like a language model that can only produce text one letter at a time. It technically works, but the model has to rediscover higher-level patterns (like common ring structures) from scratch. MOSAIC is more like generating at the word level: it learns to compose molecules from meaningful building blocks rather than individual atoms.

## Pipeline Overview

MOSAIC operates through a three-stage pipeline. First, molecular graphs are recursively coarsened into multi-level hierarchies where atoms are grouped into structurally meaningful clusters. Second, these hierarchies are serialized into structured token sequences that preserve parent-child relationships and inter-cluster connectivity. Third, a Transformer decoder is trained to autoregressively generate these structured sequences, enabling the model to compose molecules from coarse structure down to fine-grained atomic detail.

<Figure>
  <Picture slot="figure" src={pipelineOverview} alt="MOSAIC pipeline overview showing the three stages: hierarchical coarsening, structured tokenization, and autoregressive generation" />
  <Fragment slot="caption">**MOSAIC pipeline overview.** A molecular graph (camptothecin) is hierarchically coarsened into multi-level clusters, tokenized into a structured sequence encoding the hierarchy, and generated autoregressively by a Transformer decoder.</Fragment>
</Figure>

## Coarsening Strategies

MOSAIC supports multiple graph coarsening strategies that recursively partition molecular graphs into hierarchical clusters. Each strategy offers different trade-offs between preserving chemical motifs and computational efficiency.

- **Spectral Clustering**: Uses the eigenvectors of the graph Laplacian to identify natural clusters in the molecular graph. Spectral methods capture global graph structure and produce balanced partitions.

- **Hierarchical Agglomerative Clustering (HAC)**: A bottom-up approach that iteratively merges the most similar adjacent nodes based on bond-type-weighted distances. HAC tends to preserve local chemical structure.

- **Motif-aware Coarsening (MC)**: First identifies known chemical motifs (rings, functional groups) using SMARTS pattern matching, then applies spectral or HAC clustering to the remaining atoms. This ensures that chemically meaningful substructures are preserved as intact units in the hierarchy.

- **Motif-aware + Functional Group (MC+FG)**: Extends motif-aware coarsening with an expanded library of functional group patterns, providing finer-grained control over which substructures are preserved during coarsening.

<Figure>
  <Picture slot="figure" src={coarsening} alt="Comparison of coarsening strategies: Spectral, HAC, Motif-aware Spectral, and Motif-aware HAC" />
  <Fragment slot="caption">**Coarsening strategies.** Different approaches to recursively partitioning a molecular graph. Motif-aware variants (right) preserve chemically meaningful substructures like rings and functional groups as intact clusters.</Fragment>
</Figure>

## Tokenization Schemes

The coarsened hierarchies are serialized into token sequences using one of four tokenization schemes, each capturing increasing levels of hierarchical information:

- **SENT** (Segmented Eulerian Neighborhood Trails): A flat baseline that serializes atom-bond sequences without hierarchy. Serves as the non-hierarchical reference point.

- **H-SENT** (Hierarchical SENT): Extends SENT by encoding multi-level partition structure. Tokens include partition boundaries and parent-child relationships, enabling the model to learn coarse-to-fine generation.

- **HDT** (Hierarchical Depth Tokenization): Encodes the full tree structure of the hierarchy using depth-first traversal. Each token carries depth information, making the hierarchical nesting explicit.

- **HDTC** (Hierarchical Depth Tokenization with Composition): The most expressive scheme, extending HDT with explicit compositional tokens at each internal node. This gives the model complete information about the branching structure of the hierarchy.

<Figure>
  <Picture slot="figure" src={tokenization} alt="Comparison of four tokenization schemes: SENT, H-SENT, HDT, and HDTC" />
  <Fragment slot="caption">**Tokenization schemes.** From left to right: increasing hierarchical expressiveness. SENT provides a flat baseline, while H-SENT, HDT, and HDTC progressively encode more structural information about the molecular hierarchy.</Fragment>
</Figure>

## Increasing Molecular Constraints

Each tokenization strategy imposes a different level of chemical constraint on the generation process. Moving from flat random walks (SENT) through flexible coarsening (SC, HAC) to motif-constrained coarsening (MC, MC+FG), the generated molecules become progressively more structurally coherent and closer to real natural products.

<Figure>
  <Picture slot="figure" src={constraint} alt="Unconditional COCONUT generations arranged by increasing molecular constraint, from SENT through spectral coarsening to motif-constrained methods" />
  <Fragment slot="caption">**Increasing molecular constraints.** Representative COCONUT generations from each tokenization, ordered left to right by increasing chemical constraint. As the tokenization imposes stronger structural priors — from unconstrained flat walks to typed motif communities — generated molecules become more chemically coherent.</Fragment>
</Figure>

## Results

We evaluate unconditional generation on **MOSES** (~1M drug-like molecules, 10–26 heavy atoms) and **COCONUT** (~5K complex natural products, 30–100 heavy atoms, ≥4 rings). All models use a GPT-2 backbone (12 layers, 768 hidden, 12 heads, ~85M parameters) trained with identical hyperparameters. We generate 500 molecules per model and compare against the full reference dataset. **Bold** = better of SENT vs. best hierarchical per metric. Each "Best Hierarchical" cell shows *value (model)*.

<Wide>
<table class="neurips-table">
<caption><strong>Table 1.</strong> MOSES and COCONUT benchmark summary: flat-walk baseline (SENT) vs. best hierarchical tokenizer per metric. ↑ = higher is better, ↓ = lower is better.</caption>
<thead>
<tr>
<th></th>
<th colspan="2" style="text-align: center; border-bottom: none;">MOSES</th>
<th colspan="2" class="group-separator" style="text-align: center; border-bottom: none;">COCONUT</th>
</tr>
<tr class="sub-header">
<th></th>
<th>SENT</th>
<th>Best Hier.</th>
<th class="group-separator">SENT</th>
<th>Best Hier.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Validity ↑</td>
<td>0.868</td>
<td><span class="best">0.891 (HDT MC)</span></td>
<td class="group-separator">0.618</td>
<td><span class="best">0.918 (HDTC)</span></td>
</tr>
<tr>
<td>Uniqueness ↑</td>
<td><span class="best">1.000</span></td>
<td><span class="best">1.000 (all)</span></td>
<td class="group-separator"><span class="best">1.000</span></td>
<td><span class="best">1.000</span></td>
</tr>
<tr>
<td>Novelty ↑</td>
<td>0.906</td>
<td><span class="best">0.938 (H-SENT SC)</span></td>
<td class="group-separator"><span class="best">1.000</span></td>
<td><span class="best">1.000</span></td>
</tr>
<tr class="midrule">
<td>FCD ↓</td>
<td><span class="best">3.07</span></td>
<td>3.17 (HDTC)</td>
<td class="group-separator">6.94</td>
<td><span class="best">4.50 (HDT MC)</span></td>
</tr>
<tr>
<td>SNN ↑</td>
<td>0.347</td>
<td><span class="best">0.377 (HDT MC)</span></td>
<td class="group-separator">0.269</td>
<td><span class="best">0.991 (HDT MC)</span></td>
</tr>
<tr>
<td>Frag ↑</td>
<td>0.993</td>
<td><span class="best">0.994 (HDT MC)</span></td>
<td class="group-separator">0.944</td>
<td><span class="best">0.969 (HDTC)</span></td>
</tr>
<tr>
<td>Scaff ↑</td>
<td><span class="best">0.761</span></td>
<td>0.759 (HDT MC)</td>
<td class="group-separator">0.000</td>
<td><span class="best">0.150 (HDTC)</span></td>
</tr>
<tr>
<td>IntDiv ↑</td>
<td>0.864</td>
<td><span class="best">0.885 (HDT HAC)</span></td>
<td class="group-separator">0.887</td>
<td><span class="best">0.888 (HDT HAC)</span></td>
</tr>
<tr>
<td>PGD ↓</td>
<td>0.020</td>
<td><span class="best">0.000 (HDTC)</span></td>
<td class="group-separator">1.000</td>
<td>1.000 (all)</td>
</tr>
<tr class="midrule">
<td>FG MMD ↓</td>
<td><span class="best">0.002</span></td>
<td><span class="best">0.002 (H-SENT MC)</span></td>
<td class="group-separator">0.004</td>
<td><span class="best">0.002 (HDT MC)</span></td>
</tr>
<tr>
<td>SMARTS MMD ↓</td>
<td><span class="best">0.003</span></td>
<td><span class="best">0.003 (H-SENT MC)</span></td>
<td class="group-separator">0.005</td>
<td><span class="best">0.002 (HDT MC)</span></td>
</tr>
<tr>
<td>Ring MMD ↓</td>
<td>0.005</td>
<td><span class="best">0.004 (HDT MC)</span></td>
<td class="group-separator">0.012</td>
<td><span class="best">0.003 (HDT MC)</span></td>
</tr>
<tr>
<td>BRICS MMD ↓</td>
<td><span class="best">0.019</span></td>
<td>0.023 (HDTC)</td>
<td class="group-separator">0.046</td>
<td><span class="best">0.037 (HDT MC)</span></td>
</tr>
<tr>
<td>Motif Rate ↑</td>
<td>0.790</td>
<td><span class="best">0.900 (H-SENT SC)</span></td>
<td class="group-separator">0.553</td>
<td><span class="best">0.704 (HDTC)</span></td>
</tr>
</tbody>
</table>
</Wide>

On **MOSES** (simple drug-like molecules), hierarchy provides modest improvements — SENT already achieves 86.8% validity and the gap is small. On **COCONUT** (complex natural products), the picture changes dramatically: SENT drops to 61.8% validity while HDTC reaches 91.8%, a **30-point gap**. FCD improves from 6.94 to 4.50, scaffold similarity jumps from 0.000 to 0.150, and Ring MMD drops 4× from 0.012 to 0.003. The benefit of hierarchical tokenization scales with molecular complexity.

## Discussion

<div class="key-finding">

### 1. Increasing Molecular Constraint Stabilizes Training

Flexible coarsening methods (spectral clustering, HAC) are **fragile**: small hyperparameter changes cause catastrophic failure. On COCONUT with HAC coarsening, H-SENT's fragment similarity drops from 0.70 → 0.11 → 0.04 → 0.00 as learning rate increases from 6e-4 to 4e-3, while Ring MMD explodes to ~0.98. In contrast, **HDT with the same HAC coarsening remains completely stable** (Frag 0.91–0.95, FG MMD 0.009–0.012) across all four learning rates. On MOSES, H-SENT SC achieves 73–78% validity but H-SENT HAC yields only 17–20%. Motif-constrained coarsening (MC, MC+FG) eliminates this fragility by aligning partitions with chemical structure, so each ring or functional group is generated as a coherent block.

</div>

<div class="key-finding">

### 2. Abstraction & Composition Only Needed for Complex, Long-Range Graphs

On MOSES (~20 atoms, simple scaffolds), SENT already achieves 86.8% validity and the best hierarchical method improves by only 2.3 percentage points. FCD is nearly tied (3.07 vs. 3.17), and motif MMD values are uniformly low. On COCONUT (30–100 atoms, ≥4 fused rings), the story reverses: validity gap widens to **30 points**, FCD improves by 2.4 points, Ring MMD drops 4×, and scaffold similarity goes from 0.000 (complete failure) to 0.150. For small, simple molecules, a flat random walk provides sufficient context. As molecules grow larger with multiple fused ring systems and intricate connectivity, flat walks fail to convey compositional structure and hierarchical tokenization becomes essential — exactly the regime relevant to natural products and drug discovery.

</div>

<div class="key-finding">

### 3. Structural Understanding Achieves Reasonable Performance with ~3× Smaller Models

Our GPT-2 backbone uses **~85M parameters**. AutoGraph, the state-of-the-art flat-walk baseline, employs a **~256M parameter** architecture. Despite using ~3× fewer parameters, MOSAIC with hierarchical tokenization achieves comparable generation quality on MOSES and substantially better results on complex molecules (COCONUT). This suggests that the structural inductive bias from hierarchical tokenization — where the model receives explicit community structure rather than inferring it — reduces the capacity needed for high-quality generation. Hierarchical tokenization trades explicit structural encoding for implicit learning burden, allowing smaller models to match or exceed larger ones.

</div>

## Generation Gallery

<Figure>
  <Picture slot="figure" src={galleryMoses} alt="Generation gallery showing reference molecules from MOSES alongside molecules generated by each of the six MOSAIC tokenizer variants" />
  <Fragment slot="caption">**Generation gallery.** Reference molecules from MOSES (left column, gray background) paired with the closest-sized valid generation from each of six model variants. Each row targets a different molecular size, demonstrating that hierarchical tokenizers produce structurally diverse, chemically plausible molecules across the atom-count range.</Fragment>
</Figure>

## Generation Demo

The following animation shows MOSAIC's autoregressive generation process, where the model builds a molecule token-by-token, progressively assembling the hierarchical structure from coarse partitions down to individual atoms and bonds.

<Figure>
  <img slot="figure" src={new URL("demo.gif", import.meta.env.SITE + import.meta.env.BASE_URL + "/").pathname} alt="Animated demonstration of MOSAIC's autoregressive molecular generation process" style="width: 100%; border-radius: 0.5rem;" />
  <Fragment slot="caption">**Autoregressive generation demo.** MOSAIC generates a molecule by sequentially predicting tokens that encode hierarchical structure, atom types, and bond connectivity.</Fragment>
</Figure>

## BibTeX Citation

```bibtex
@article{bian2025mosaic,
  title     = {Beyond Flat Walks: Compositional Abstraction for
               Autoregressive Molecular Generation},
  author    = {Bian, Kaiwen and Yang, Andrew H. and Parviz, Ali
               and Mishne, Gal and Wang, Yusu},
  year      = {2025},
  url       = {https://github.com/KevinBian107/MOSAIC},
}
```
