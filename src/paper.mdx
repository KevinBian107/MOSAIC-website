---
title: "Beyond Flat Walks: Compositional Abstraction for Autoregressive Molecular Generation"
authors:
  - name: Kaiwen Bian
  - name: Andrew H. Yang
  - name: Ali Parviz
  - name: Gal Mishne
  - name: Yusu Wang
links:
  - name: Paper
    url: "#"
    icon: ri:file-pdf-2-line
  - name: Code
    url: https://github.com/KevinBian107/MOSAIC
    icon: ri:github-line
theme: device
favicon: favicon.svg
description: "MOSAIC: Compositional Abstraction for Autoregressive Molecular Generation"
---

import HighlightedSection from "./components/HighlightedSection.astro";
import Figure from "./components/Figure.astro";
import Picture from "./components/Picture.astro";
import Wide from "./components/Wide.astro";

import pipelineOverview from "./assets/pipeline_overview.png";
import coarsening from "./assets/coarsening.png";
import tokenization from "./assets/tokenization.png";

<HighlightedSection>

## Abstract

Autoregressive models for molecular graph generation typically operate on flattened sequences of atoms and bonds, discarding the rich multi-scale structure inherent to molecules. We introduce **MOSAIC** (**M**ulti-scale **O**rganization via **S**tructural **A**bstraction **I**n **C**omposition), a framework that lifts autoregressive generation from flat token walks to compositional, hierarchy-aware sequences. MOSAIC provides a unified three-stage pipeline: (1) **hierarchical coarsening** that recursively groups atoms into motif-like clusters using graph-theoretic methods (spectral clustering, hierarchical agglomerative clustering, and motif-aware variants), (2) **structured tokenization** that serializes the resulting multi-level hierarchy into sequences that explicitly encode parent-child relationships, partition boundaries, and edge connectivity at every level, and (3) **autoregressive generation** with a standard Transformer decoder that learns to produce these structured sequences. We evaluate MOSAIC on the MOSES and COCONUT molecular benchmarks, comparing four tokenization schemes of increasing hierarchical expressiveness. Our experiments show that hierarchy-aware tokenizations improve chemical validity and structural diversity over flat baselines while enabling control over generated substructures. MOSAIC provides a principled, modular foundation for structure-aware molecular generation.

</HighlightedSection>

## Pipeline Overview

MOSAIC operates through a three-stage pipeline. First, molecular graphs are recursively coarsened into multi-level hierarchies where atoms are grouped into structurally meaningful clusters. Second, these hierarchies are serialized into structured token sequences that preserve parent-child relationships and inter-cluster connectivity. Third, a Transformer decoder is trained to autoregressively generate these structured sequences, enabling the model to compose molecules from coarse structure down to fine-grained atomic detail.

<Wide>
  <Figure>
    <Picture slot="figure" src={pipelineOverview} alt="MOSAIC pipeline overview showing the three stages: hierarchical coarsening, structured tokenization, and autoregressive generation" />
    <Fragment slot="caption">**MOSAIC pipeline overview.** A molecular graph (camptothecin) is hierarchically coarsened into multi-level clusters, tokenized into a structured sequence encoding the hierarchy, and generated autoregressively by a Transformer decoder.</Fragment>
  </Figure>
</Wide>

## Generation Demo

The following animation shows MOSAIC's autoregressive generation process, where the model builds a molecule token-by-token, progressively assembling the hierarchical structure from coarse partitions down to individual atoms and bonds.

<Wide>
  <Figure>
    <img slot="figure" src={`${import.meta.env.BASE_URL}demo.gif`} alt="Animated demonstration of MOSAIC's autoregressive molecular generation process" style="width: 100%; border-radius: 0.5rem;" />
    <Fragment slot="caption">**Autoregressive generation demo.** MOSAIC generates a molecule by sequentially predicting tokens that encode hierarchical structure, atom types, and bond connectivity.</Fragment>
  </Figure>
</Wide>

## Coarsening Strategies

MOSAIC supports multiple graph coarsening strategies that recursively partition molecular graphs into hierarchical clusters. Each strategy offers different trade-offs between preserving chemical motifs and computational efficiency.

- **Spectral Clustering**: Uses the eigenvectors of the graph Laplacian to identify natural clusters in the molecular graph. Spectral methods capture global graph structure and produce balanced partitions.

- **Hierarchical Agglomerative Clustering (HAC)**: A bottom-up approach that iteratively merges the most similar adjacent nodes based on bond-type-weighted distances. HAC tends to preserve local chemical structure.

- **Motif-aware Coarsening (MC)**: First identifies known chemical motifs (rings, functional groups) using SMARTS pattern matching, then applies spectral or HAC clustering to the remaining atoms. This ensures that chemically meaningful substructures are preserved as intact units in the hierarchy.

- **Motif-aware + Functional Group (MC+FG)**: Extends motif-aware coarsening with an expanded library of functional group patterns, providing finer-grained control over which substructures are preserved during coarsening.

<Wide>
  <Figure>
    <Picture slot="figure" src={coarsening} alt="Comparison of coarsening strategies: Spectral, HAC, Motif-aware Spectral, and Motif-aware HAC" />
    <Fragment slot="caption">**Coarsening strategies.** Different approaches to recursively partitioning a molecular graph. Motif-aware variants (right) preserve chemically meaningful substructures like rings and functional groups as intact clusters.</Fragment>
  </Figure>
</Wide>

## Tokenization Schemes

The coarsened hierarchies are serialized into token sequences using one of four tokenization schemes, each capturing increasing levels of hierarchical information:

- **SENT** (Sequential Edge-Node Tokenization): A flat baseline that serializes atom-bond sequences without hierarchy. Serves as the non-hierarchical reference point.

- **H-SENT** (Hierarchical SENT): Extends SENT by encoding multi-level partition structure. Tokens include partition boundaries and parent-child relationships, enabling the model to learn coarse-to-fine generation.

- **HDT** (Hierarchical Depth Tokenization): Encodes the full tree structure of the hierarchy using depth-first traversal. Each token carries depth information, making the hierarchical nesting explicit.

- **HDTC** (Hierarchical Depth Tokenization with Children): The most expressive scheme, extending HDT with explicit child-count tokens at each internal node. This gives the model complete information about the branching structure of the hierarchy.

<Wide>
  <Figure>
    <Picture slot="figure" src={tokenization} alt="Comparison of four tokenization schemes: SENT, H-SENT, HDT, and HDTC" />
    <Fragment slot="caption">**Tokenization schemes.** From left to right: increasing hierarchical expressiveness. SENT provides a flat baseline, while H-SENT, HDT, and HDTC progressively encode more structural information about the molecular hierarchy.</Fragment>
  </Figure>
</Wide>

## BibTeX Citation

```bibtex
@article{bian2025mosaic,
  title     = {Beyond Flat Walks: Compositional Abstraction for
               Autoregressive Molecular Generation},
  author    = {Bian, Kaiwen and Yang, Andrew H. and Parviz, Ali
               and Mishne, Gal and Wang, Yusu},
  year      = {2025},
  url       = {https://github.com/KevinBian107/MOSAIC},
}
```
